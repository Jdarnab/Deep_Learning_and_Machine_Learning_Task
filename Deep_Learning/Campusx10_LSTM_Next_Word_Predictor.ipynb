{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2OIjuZgmOAif"
      },
      "outputs": [],
      "source": [
        "# !pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od"
      ],
      "metadata": {
        "id": "9jpVZvwuhOpk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "od.download_kaggle_dataset('https://www.kaggle.com/datasets/ashishpandey2062/next-word-predictor-text-generator-dataset', data_dir= '/content/sample_data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsJcXtArhRVL",
        "outputId": "268b95a4-d5a5-4020-bf64-1babb7e75c6a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/ashishpandey2062/next-word-predictor-text-generator-dataset\n",
            "Downloading next-word-predictor-text-generator-dataset.zip to /content/sample_data/next-word-predictor-text-generator-dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 61.5k/61.5k [00:00<00:00, 111MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Docs creation"
      ],
      "metadata": {
        "id": "Pz_A5cPq8Vhu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc5646a8"
      },
      "source": [
        "# import os\n",
        "# download_path = '/content/sample_data/next-word-predictor-text-generator-dataset'\n",
        "# if os.path.exists(download_path):\n",
        "#     print(f\"Contents of {download_path}:\")\n",
        "#     for item in os.listdir(download_path):\n",
        "#         print(item)\n",
        "# else:\n",
        "#     print(f\"Directory not found: {download_path}\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d6a1cfb",
        "outputId": "8c567c7e-4ad0-427d-e9d5-213d97fbc56c"
      },
      "source": [
        "# Define the path to the downloaded text file\n",
        "file_path = '/content/sample_data/next-word-predictor-text-generator-dataset/next_word_predictor.txt'\n",
        "\n",
        "# Read the content of the file\n",
        "with open(file_path, 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Print the first 500 characters and the total size of the text\n",
        "print(\"First 500 characters:\")\n",
        "print(text[:500])\n",
        "print(\"\\nTotal size of the text:\", len(text), \"characters\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 500 characters:\n",
            "The sun was shining brightly in the clear blue sky, and a gentle breeze rustled the leaves of the tall trees. People were out enjoying the beautiful weather, some sitting in the park, others taking a leisurely stroll along the riverbank. Children were playing games, and laughter filled the air.\n",
            "\n",
            "As the day turned into evening, the temperature started to drop, and the sky transformed into a canvas of vibrant colors. Families gathered for picnics, and the smell of barbecues wafted through the air.\n",
            "\n",
            "Total size of the text: 167445 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4ad2ba8"
      },
      "source": [
        "## Preprocess the data\n",
        "\n",
        "Clean and prepare the text data for model training, including tokenization and creating sequences."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "tokenizer= Tokenizer ()\n",
        "tokenizer.fit_on_texts([text])"
      ],
      "metadata": {
        "id": "kpsXQxTNk0z9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len( tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKYk2Fc65RFo",
        "outputId": "f8fe5cb5-fbd3-46ba-a57d-d87cfc99df14"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4993"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Calculate the maximum sequence length after tokenization\n",
        "# max_sequence_len = max([len(tokenizer_sentences= tokenizer.texts_to_sequences([line])[0]) for line in text.split('\\n') if line.strip()])\n",
        "\n",
        "# print(f\"Maximum length of tokenized sequences: {max_sequence_len}\")\n",
        "tokenized_sentences= [tokenizer.texts_to_sequences([line])[0]\n",
        "                      for line in text.split('\\n')\n",
        "                       if line.strip()]\n",
        "print(f\"Maximum length: {max(len(seq) for seq in tokenized_sentences)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fb-x2vXxx5c2",
        "outputId": "610b63b1-454c-4ce0-a762-70ae0ad8d22c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum length: 325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences=[]\n",
        "for sentence_tokens in tokenized_sentences:\n",
        "  for i in range (1, len(sentence_tokens)):\n",
        "    input_sequences.append (sentence_tokens[:i+1])"
      ],
      "metadata": {
        "id": "WfLOjYgSoby7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max([len(seq) for seq in input_sequences])\n",
        "print(f\"Maximum sequence length: {max_len}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRamT7ZnqfMf",
        "outputId": "5482728b-bec1-40a3-a131-d719d520e933"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum sequence length: 325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_input_sequences= pad_sequences (input_sequences, maxlen= max_len, padding='pre')\n",
        "padded_input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3V73lMqq2oi",
        "outputId": "ec57a549-727d-481f-a25d-7a54eeecaab7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,    0,    1,  155],\n",
              "       [   0,    0,    0, ...,    1,  155,   21],\n",
              "       [   0,    0,    0, ...,  155,   21, 2368],\n",
              "       ...,\n",
              "       [   0,    0,    0, ..., 2331,  290,   19],\n",
              "       [   0,    0,    0, ...,  290,   19,   54],\n",
              "       [   0,    0,    0, ...,   19,   54, 1535]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_input_sequences.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzlnlHKarkNT",
        "outputId": "e48e0925-f6cc-4ad8-864c-2ed9fd13dc57"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26383, 325)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X= padded_input_sequences [:,:-1]\n",
        "y= padded_input_sequences[:,-1]"
      ],
      "metadata": {
        "id": "ZS3d4Iy-34Yu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y= to_categorical(y, num_classes= len(tokenizer.word_index)+1) ## converted it into a one hot encoded type output\n",
        "y.shape , X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B45PCh2z5A-V",
        "outputId": "62e3221b-19d6-4efe-fa7c-d57ffb5b4766"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((26383, 4994), (26383, 324))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "GqDOjKg4QVY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Now the Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ],
      "metadata": {
        "id": "avncoGFl6YRr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential ()\n",
        "model.add (Embedding (input_dim=len(tokenizer.word_index)+1, output_dim=150, input_length= max_len-1))\n",
        "model.add(LSTM(150))\n",
        "model.add (Dense(len(tokenizer.word_index)+1, activation='softmax'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr_7jNup6nKC",
        "outputId": "60ed6935-cacf-4dfc-bec8-48a09d94b925"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile (loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "8dafzSyd-sgA",
        "outputId": "600925d7-04b3-49cd-91cd-7e34c5ab29a0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "0DLHl6bX_Cvk"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train,y_train, epochs=20, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEM-x664_Osr",
        "outputId": "e1ee9bf5-7699-4745-9ce5-57dca19425c6"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 22ms/step - accuracy: 0.0512 - loss: 7.3182 - val_accuracy: 0.0606 - val_loss: 6.8793\n",
            "Epoch 2/20\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.0718 - loss: 6.4360 - val_accuracy: 0.0758 - val_loss: 6.7521\n",
            "Epoch 3/20\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.0936 - loss: 5.9511 - val_accuracy: 0.0881 - val_loss: 6.7104\n",
            "Epoch 4/20\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.1130 - loss: 5.5442 - val_accuracy: 0.0995 - val_loss: 6.7038\n",
            "Epoch 5/20\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.1331 - loss: 5.1693 - val_accuracy: 0.1110 - val_loss: 6.7347\n",
            "Epoch 6/20\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.1595 - loss: 4.7704 - val_accuracy: 0.1133 - val_loss: 6.7972\n",
            "Epoch 7/20\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.1866 - loss: 4.3982 - val_accuracy: 0.1209 - val_loss: 6.8755\n",
            "Epoch 8/20\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.2256 - loss: 4.0348 - val_accuracy: 0.1224 - val_loss: 6.9685\n",
            "Epoch 9/20\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.2686 - loss: 3.7043 - val_accuracy: 0.1234 - val_loss: 7.0730\n",
            "Epoch 10/20\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.3192 - loss: 3.4228 - val_accuracy: 0.1245 - val_loss: 7.1975\n",
            "Epoch 11/20\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.3776 - loss: 3.0923 - val_accuracy: 0.1226 - val_loss: 7.3075\n",
            "Epoch 12/20\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.4463 - loss: 2.8055 - val_accuracy: 0.1247 - val_loss: 7.4552\n",
            "Epoch 13/20\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.5004 - loss: 2.5126 - val_accuracy: 0.1253 - val_loss: 7.5836\n",
            "Epoch 14/20\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.5395 - loss: 2.2830 - val_accuracy: 0.1268 - val_loss: 7.7056\n",
            "Epoch 15/20\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.5900 - loss: 2.0546 - val_accuracy: 0.1277 - val_loss: 7.8513\n",
            "Epoch 16/20\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.6296 - loss: 1.8649 - val_accuracy: 0.1300 - val_loss: 8.0050\n",
            "Epoch 17/20\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.6620 - loss: 1.6876 - val_accuracy: 0.1317 - val_loss: 8.1505\n",
            "Epoch 18/20\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.6960 - loss: 1.5248 - val_accuracy: 0.1287 - val_loss: 8.2995\n",
            "Epoch 19/20\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.7212 - loss: 1.4007 - val_accuracy: 0.1300 - val_loss: 8.4171\n",
            "Epoch 20/20\n",
            "\u001b[1m660/660\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.7490 - loss: 1.2720 - val_accuracy: 0.1309 - val_loss: 8.5708\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f79d794fed0>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "text= 'temperature'\n",
        "#tokenize\n",
        "token_text= tokenizer.texts_to_sequences ([text])[0] ##[1552]\n",
        "#padding\n",
        "padded_text= pad_sequences ([token_text], maxlen= max_len -1, padding='pre' ) ## as in input we remove one column which is y\n",
        "#predict\n",
        "y_pred= np.argmax(model.predict(padded_text))\n",
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M06NqC2cAlVC",
        "outputId": "9883d3c2-6058-4e34-ef0a-9fee402fbd2a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(21)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word, index in tokenizer.word_index.items():\n",
        "  if index == y_pred:\n",
        "    print(f'The predicted word is: {word}')\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVBhdWzqCjcU",
        "outputId": "002f3350-1770-4646-ece5-54fe9fbfead8"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The predicted word is: where\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "#test\n",
        "text= 'blue'\n",
        "for i in range (10):\n",
        "  #tokenize\n",
        "  token_text= tokenizer.texts_to_sequences ([text])[0] ##[1552]\n",
        "  #padding\n",
        "  padded_text= pad_sequences ([token_text], maxlen= max_len -1, padding='pre' ) ## as in input we remove one column which is y\n",
        "  #predict\n",
        "  y_pred= np.argmax(model.predict(padded_text))\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == y_pred:\n",
        "      text = text + \" \" + word  # Append the predicted word to the text\n",
        "      print(f'The next sentence is: {text}')\n",
        "      time.sleep (1)\n",
        "      break\n",
        "print(\"\\nGenerated text:\", text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLhDwBheDM08",
        "outputId": "9d838126-7b0d-4f51-b3e4-dd3243643391"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "The next sentence is: blue spirit\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "The next sentence is: blue spirit of\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "The next sentence is: blue spirit of innovation\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "The next sentence is: blue spirit of innovation resonates\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "The next sentence is: blue spirit of innovation resonates its\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "The next sentence is: blue spirit of innovation resonates its challenges\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "The next sentence is: blue spirit of innovation resonates its challenges for\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "The next sentence is: blue spirit of innovation resonates its challenges for its\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "The next sentence is: blue spirit of innovation resonates its challenges for its beauty\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "The next sentence is: blue spirit of innovation resonates its challenges for its beauty where\n",
            "\n",
            "Generated text: blue spirit of innovation resonates its challenges for its beauty where\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dee7e37f"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kI8l9LU4xQ9Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}